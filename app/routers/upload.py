from fastapi import APIRouter, UploadFile, File, HTTPException, BackgroundTasks, Form, Depends, Request
from fastapi.responses import JSONResponse
from app.security import get_current_user, verify_and_rotate_csrf_from_request, enforce_api_rate_limit
from app.config import APP_MODE, is_demo_mode
from app.services.blob_service import upload_to_blob, save_processed_json
from app.services.document_service import extract_text_from_url, extract_text_from_docx
from app.services.search_service import add_document_to_index, get_document_count, get_all_documents
import uuid
import traceback
from typing import Optional
import os
import re
from app.state import task_manager
from app.services.openai_service import analyze_text_for_search
from app.services.search_service import index_processed_chunks
from app.services.llm_override import LLMOverrideConfig, parse_llm_override_from_request, summarize_override_for_log
import json

router = APIRouter()

MAX_UPLOAD_BYTES = int(os.getenv("MAX_UPLOAD_BYTES", str(20 * 1024 * 1024)))
MAX_FILENAME_LENGTH = 255
INDEX_NAME_PATTERN = re.compile(r"^[a-z0-9][a-z0-9_-]{1,62}$")
ALLOWED_UPLOAD_EXTENSIONS = {
    "txt",
    "text",
    "md",
    "pdf",
    "docx",
    "py",
    "js",
    "java",
    "c",
    "cpp",
    "h",
    "cs",
    "ts",
    "tsx",
    "html",
    "css",
    "json",
}


def normalize_index_name(index_name: Optional[str]) -> Optional[str]:
    raw = (index_name or "").strip()
    if not raw:
        return None
    normalized = raw.lower()
    if not INDEX_NAME_PATTERN.match(normalized):
        raise HTTPException(
            status_code=400,
            detail="index_name í˜•ì‹ì´ ìœ íš¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. (ì˜ë¬¸ ì†Œë¬¸ì/ìˆ«ì/`-`/`_`, 2~63ì)",
        )
    return normalized


def validate_upload_input(file_name: str, file_data: bytes, file_ext: str) -> None:
    if not file_name:
        raise HTTPException(status_code=400, detail="íŒŒì¼ëª…ì´ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")
    if len(file_name) > MAX_FILENAME_LENGTH:
        raise HTTPException(status_code=400, detail=f"íŒŒì¼ëª… ê¸¸ì´ëŠ” {MAX_FILENAME_LENGTH}ìë¥¼ ì´ˆê³¼í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    if len(file_data) <= 0:
        raise HTTPException(status_code=400, detail="ë¹ˆ íŒŒì¼ì€ ì—…ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    if len(file_data) > MAX_UPLOAD_BYTES:
        raise HTTPException(
            status_code=413,
            detail=f"íŒŒì¼ í¬ê¸° ì œí•œ({MAX_UPLOAD_BYTES // (1024 * 1024)}MB)ì„ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤.",
        )

    if file_ext not in ALLOWED_UPLOAD_EXTENSIONS:
        raise HTTPException(
            status_code=415,
            detail=f"ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤: .{file_ext or 'unknown'}",
        )

# --- Async File Processing Pipeline ---

#ì°½í›ˆ ì½”ë“œ ì¶”ê°€

async def process_file_background(
    task_id: str,
    file_name: str,
    file_data: bytes,
    file_ext: str,
    index_name: str = None,
    llm_override: Optional[LLMOverrideConfig] = None,
):
    """
    ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‹¤í–‰ë  ì‹¤ì œ íŒŒì´í”„ë¼ì¸ ë¡œì§
    1. Blob ì—…ë¡œë“œ (Raw)
    2. í…ìŠ¤íŠ¸ ì¶”ì¶œ
    3. LLM ì „ì²˜ë¦¬ (JSON ìƒì„±)
    4. Blob ì—…ë¡œë“œ (Processed JSON)
    5. Azure Search ì¸ë±ì‹±

    Args:
        index_name: RAG ì¸ë±ìŠ¤ ì´ë¦„ (ì§€ì •í•˜ì§€ ì•Šìœ¼ë©´ ê¸°ë³¸ ì¸ë±ìŠ¤ ì‚¬ìš©)
    """
    try:
        print(f"[Background] Processing task {task_id} for file {file_name}...")
        task_manager.update_task(task_id, status="processing", progress=10, message=f"Uploading raw file: {file_name}")

        # Demo-mode: local extraction -> local/demo chunking -> in-memory index (no cloud creds needed).
        if is_demo_mode():
            from app.services.demo_pipeline import build_demo_chunks
            from app.services import demo_store

            task_manager.update_task(task_id, progress=20, message=f"[Demo mode] Extracting text: {file_name}")

            extracted_text = ""
            if file_ext in ['txt', 'py', 'js', 'java', 'c', 'cpp', 'h', 'cs', 'ts', 'tsx', 'html', 'css', 'json', 'md']:
                try:
                    extracted_text = file_data.decode('utf-8')
                except UnicodeDecodeError:
                    extracted_text = file_data.decode('cp949', errors='ignore')
            elif file_ext == 'docx':
                try:
                    extracted_text = extract_text_from_docx(file_data)
                except Exception as e:
                    task_manager.update_task(task_id, status="failed", message=f"[Demo mode] DOCX extraction failed: {str(e)}")
                    return
            else:
                task_manager.update_task(
                    task_id,
                    status="failed",
                    message=f"[Demo mode] Unsupported file type: .{file_ext or 'unknown'}. "
                    f"Demo mode supports txt/md/code/docx. Use live mode for PDF/images.",
                )
                return

            if not extracted_text.strip():
                task_manager.update_task(task_id, status="failed", message="[Demo mode] No text extracted from file.")
                return

            file_type = "code" if file_ext in ['py', 'js', 'java', 'cpp', 'ts', 'tsx', 'cs'] else "doc"
            if llm_override:
                task_manager.update_task(task_id, progress=60, message="[Demo mode] Preprocessing with user LLM...")
                chunks = analyze_text_for_search(
                    extracted_text,
                    file_name,
                    file_type=file_type,
                    llm_override=llm_override,
                )
            else:
                task_manager.update_task(task_id, progress=60, message="[Demo mode] Preprocessing (chunking)...")
                chunks = build_demo_chunks(extracted_text, file_name, file_type=file_type)
            if not chunks:
                task_manager.update_task(task_id, status="failed", message="[Demo mode] Preprocessing failed (no chunks).")
                return

            task_manager.update_task(task_id, progress=85, message="[Demo mode] Indexing locally...")
            indexed_count = demo_store.add_chunks(index_name, chunks)
            task_manager.update_task(
                task_id,
                status="completed",
                progress=100,
                message=f"[Demo mode] Upload complete. Indexed {indexed_count} chunks to '{demo_store.normalize_index_name(index_name)}'.",
            )
            return
        
        # 1. Blob ì—…ë¡œë“œ (Raw)
        # ì¤‘ìš”: íŒŒì¼ëª…ì— í•œê¸€/íŠ¹ìˆ˜ë¬¸ì/ê³µë°±ì´ ìˆìœ¼ë©´ Document Intelligenceê°€ URL ë‹¤ìš´ë¡œë“œì— ì‹¤íŒ¨í•¨.
        # ë”°ë¼ì„œ Blob ì €ì¥ ì‹œì—ëŠ” ì•ˆì „í•œ ì˜ë¬¸ ì´ë¦„(Task ID)ì„ ì‚¬ìš©í•˜ê³ , ì›ë³¸ íŒŒì¼ëª…ì€ ë©”íƒ€ë°ì´í„°ë¡œë§Œ ê´€ë¦¬í•¨.
        safe_file_name = f"{task_id}.{file_ext}" if file_ext else task_id

        try:
            # upload_to_blobì€ ì´ë¯¸ SAS Tokenì´ í¬í•¨ëœ URLì„ ë°˜í™˜í•¨
            blob_url_with_sas = upload_to_blob(safe_file_name, file_data, index_name=index_name)
            print(f"[Background] Blob upload success: {blob_url_with_sas}")
            
        except Exception as e:
            print(f"[Background] Blob upload failed: {e}")
            raise e

        task_manager.update_task(task_id, progress=30, message="Extracting text...")
        
        # 2. í…ìŠ¤íŠ¸ ì¶”ì¶œ
        extracted_text = ""
        if file_ext in ['txt', 'py', 'js', 'java', 'c', 'cpp', 'h', 'cs', 'ts', 'tsx', 'html', 'css', 'json', 'md']:
            # í…ìŠ¤íŠ¸/ì½”ë“œ íŒŒì¼ì€ ì§ì ‘ ë””ì½”ë”©
            try:
                extracted_text = file_data.decode('utf-8')
            except UnicodeDecodeError:
                extracted_text = file_data.decode('cp949', errors='ignore')
        elif file_ext == 'docx':
            # DOCX ë¡œì»¬ ì¶”ì¶œ (ë¹ ë¥´ê³  ë¬´ë£Œ, URL ì—ëŸ¬ ì—†ìŒ)
            print("[Background] File is DOCX. Attempting local extraction...")
            try:
                extracted_text = extract_text_from_docx(file_data)
                print(f"[Background] DOCX extraction success. Length: {len(extracted_text)}")
            except Exception as e:
                print(f"[Background] DOCX extraction failed: {e}")
                task_manager.update_task(task_id, status="failed", message=f"DOCX extraction failed: {str(e)}")
                return
        else:
            # PDF, ì´ë¯¸ì§€ ë“±ì€ Document Intelligence ì‚¬ìš© (SAS Token í¬í•¨ URL ì‚¬ìš©)
            try:
                extracted_text = extract_text_from_url(blob_url_with_sas)
            except Exception as e:
                task_manager.update_task(task_id, status="failed", message=f"Text extraction failed: {str(e)}")
                return

        if not extracted_text:
            task_manager.update_task(task_id, status="failed", message="No text extracted from file.")
            return
            
        task_manager.update_task(task_id, progress=50, message="Analyzing with AI (Preprocessing)...")
        print("[Background] Starting LLM analysis...")

        # 3. LLM ì „ì²˜ë¦¬
        # íŒŒì¼ ìœ í˜• êµ¬ë¶„ (code vs doc)
        file_type = "code" if file_ext in ['py', 'js', 'java', 'cpp', 'ts', 'tsx', 'cs'] else "doc"
        
        # print(f"extracted_text : {extracted_text}")
        chunks = analyze_text_for_search(
            extracted_text,
            file_name,
            file_type=file_type,
            llm_override=llm_override,
        )
        print(f"[Background] LLM analysis returned {len(chunks) if chunks else 0} chunks.")
        
        if not chunks:
            task_manager.update_task(task_id, status="failed", message="AI preprocessing failed (No chunks generated).")
            return

        task_manager.update_task(task_id, progress=70, message="Saving processed data...")

        # 4. Processed JSON ì €ì¥ (Blob)
        # JSON íŒŒì¼ëª…ë„ ì•ˆì „í•˜ê²Œ Task ID ê¸°ë°˜ìœ¼ë¡œ ì €ì¥
        processed_file_name = f"{task_id}_processed.json"
        try:
            json_str = json.dumps(chunks, ensure_ascii=False, indent=2)
            save_processed_json(processed_file_name, json_str, index_name=index_name)
        except Exception as e:
            print(f"âš ï¸ Failed to save processed json: {e}")
            # ì €ì¥ì€ ì‹¤íŒ¨í•´ë„ ì§„í–‰

        task_manager.update_task(task_id, progress=80, message="Indexing to Search...")

        # 5. Azure Search ì¸ë±ì‹±
        print(f"[Background] Starting indexing for {len(chunks)} chunks to index '{index_name or 'default'}'...")
        try:
            indexed_count = index_processed_chunks(chunks, index_name=index_name)
            print(f"[Background] Indexing complete. Count: {indexed_count}")
        except Exception as e:
            print(f"[Background] Indexing failed: {e}")
            raise e
        
        if indexed_count > 0:
            task_manager.update_task(task_id, status="completed", progress=100, message="Upload & Indexing Complete!")
        else:
            task_manager.update_task(task_id, status="completed_with_warning", progress=100, message="Finished, but no documents indexed.")

    except Exception as e:
        print(f"âŒ Background task failed: {e}")
        traceback.print_exc()
        task_manager.update_task(task_id, status="failed", message=f"Internal Server Error: {str(e)}")


@router.post("")
async def upload_document(
    request: Request,
    background_tasks: BackgroundTasks,
    file: UploadFile = File(...),
    index_name: str = Form(None),
    user: dict = Depends(get_current_user)
):
    new_csrf_token = verify_and_rotate_csrf_from_request(request, user["email"])
    enforce_api_rate_limit(
        request,
        bucket="upload-post",
        limit=20,
        window_seconds=60,
        user_email=user.get("email"),
    )
    """
    íŒŒì¼ ì—…ë¡œë“œ ì—”ë“œí¬ì¸íŠ¸ (ë¹„ë™ê¸° ì²˜ë¦¬)
    íŒŒì¼ì„ ë°›ìë§ˆì task_idë¥¼ ë¦¬í„´í•˜ê³ , ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì²˜ë¦¬ ì‹œì‘.

    Args:
        file: ì—…ë¡œë“œí•  íŒŒì¼
        index_name: RAG ì¸ë±ìŠ¤ ì´ë¦„ (ì„ íƒ ì‚¬í•­, ì§€ì •í•˜ì§€ ì•Šìœ¼ë©´ ê¸°ë³¸ ì¸ë±ìŠ¤)
    """
    try:
        normalized_index_name = normalize_index_name(index_name)
        llm_override = parse_llm_override_from_request(request)
        # 1. íŒŒì¼ ë°ì´í„° ì½ê¸° (ë©”ëª¨ë¦¬)
        file_data = await file.read()
        file_name = file.filename
        file_ext = file_name.lower().split('.')[-1] if '.' in file_name else ''
        validate_upload_input(file_name, file_data, file_ext)

        # 2. Task ìƒì„±
        task_id = str(uuid.uuid4())
        task_manager.create_task(task_id, owner_email=user.get("email", ""))

        # 3. ë°±ê·¸ë¼ìš´ë“œ ì‘ì—… ë“±ë¡
        print(
            f"ğŸ“‹ Upload request: file={file_name}, index={normalized_index_name or 'default'}, "
            f"llm={summarize_override_for_log(llm_override)}"
        )
        background_tasks.add_task(
            process_file_background,
            task_id,
            file_name,
            file_data,
            file_ext,
            normalized_index_name,
            llm_override,
        )

        return JSONResponse(
            content={
                "message": "Upload started",
                "task_id": task_id,
                "file_name": file_name,
                "index_name": normalized_index_name or "default",
            },
            headers={"X-CSRF-Token": new_csrf_token},
        )

    except HTTPException as exc:
        headers = dict(exc.headers or {})
        headers["X-CSRF-Token"] = new_csrf_token
        raise HTTPException(status_code=exc.status_code, detail=exc.detail, headers=headers)
        
    except Exception as e:
        print(f"âŒ Upload request failed: {e}")
        traceback.print_exc()
        raise HTTPException(
            status_code=500,
            detail="íŒŒì¼ ì—…ë¡œë“œ ìš”ì²­ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
            headers={"X-CSRF-Token": new_csrf_token},
        )


@router.get("/status/{task_id}")
async def get_task_status(request: Request, task_id: str, user: dict = Depends(get_current_user)):
    """ë°±ê·¸ë¼ìš´ë“œ ì‘ì—… ìƒíƒœ ì¡°íšŒ"""
    enforce_api_rate_limit(
        request,
        bucket="upload-status",
        limit=300,
        window_seconds=60,
        user_email=user.get("email"),
    )
    task = task_manager.get_task(task_id, include_private=True)
    if task is None:
        raise HTTPException(status_code=404, detail="Task not found")

    email = user.get("email")
    role = user.get("role")
    if role != "admin" and task.get("owner_email") != email:
        raise HTTPException(status_code=403, detail="í•´ë‹¹ ì‘ì—…ì— ì ‘ê·¼í•  ê¶Œí•œì´ ì—†ìŠµë‹ˆë‹¤.")

    safe_task = dict(task)
    safe_task.pop("owner_email", None)
    return safe_task



@router.get("/stats")
async def get_stats(request: Request, index_name: str = "documents-index", _user: dict = Depends(get_current_user)):
    """ì‹œìŠ¤í…œ í†µê³„ ì¡°íšŒ - ìµœê·¼ ì—…ë¡œë“œ ê°¯ìˆ˜, ì¸ë±ìŠ¤ ë¬¸ì„œ ê°¯ìˆ˜"""
    try:
        enforce_api_rate_limit(
            request,
            bucket="upload-stats",
            limit=120,
            window_seconds=60,
            user_email=_user.get("email"),
        )
        index_name = normalize_index_name(index_name) or "documents-index"
        doc_count = get_document_count(index_name)
        print(f"ğŸ“Š ì‹œìŠ¤í…œ í†µê³„: {doc_count}ê°œ ë¬¸ì„œ ì¸ë±ì‹±ë¨")
        
        return {
            "total_documents": doc_count,
            "recent_uploads": doc_count,  # AI Searchì— ì¸ë±ì‹±ëœ ëª¨ë“  ë¬¸ì„œ
            "status": "ğŸ§ª Demo" if APP_MODE == "demo" else "âœ… Active",
            "index_name": index_name,
            "mode": APP_MODE,
        }
    except Exception as e:
        print(f"âŒ Stats error: {e}")
        return {
            "total_documents": 0,
            "recent_uploads": 0,
            "status": "âš ï¸ Error",
            "index_name": index_name,
            "mode": APP_MODE,
        }

@router.get("/documents")
async def list_documents(request: Request, index_name: str = "documents-index", _user: dict = Depends(get_current_user)):
    """AI Search ì¸ë±ìŠ¤ì— ì €ì¥ëœ ëª¨ë“  ë¬¸ì„œ ëª©ë¡ ì¡°íšŒ - ì‹¤ì œ content í¬í•¨"""
    try:
        enforce_api_rate_limit(
            request,
            bucket="upload-documents",
            limit=120,
            window_seconds=60,
            user_email=_user.get("email"),
        )
        index_name = normalize_index_name(index_name) or "documents-index"
        if is_demo_mode():
            from app.services import demo_store

            docs = demo_store.get_all_documents(index_name)
            out = []
            for d in docs:
                file_name = d.get("fileName") or d.get("file_name") or "Unknown"
                content = str(d.get("content") or "")
                out.append(
                    {
                        "id": d.get("id") or "",
                        "file_name": file_name,
                        "content": content,
                        "content_length": len(content),
                    }
                )

            return {"count": len(out), "documents": out}

        from app.services.search_service import get_search_client
        
        search_client = get_search_client(index_name=index_name)
        results = search_client.search(search_text="*", include_total_count=True, top=100)
        
        docs = []
        for result in results:
            file_name = result.get("fileName") or result.get("file_name") or "Unknown"
            docs.append({
                "id": result.get("id", ""),
                # Frontend expects snake_case.
                "file_name": file_name,
                "content": result.get("content", ""),  # ì‹¤ì œ content í¬í•¨!
                "content_length": len(result.get("content", ""))
            })
        
        print(f"ğŸ“‹ API ì‘ë‹µ: {len(docs)}ê°œ ë¬¸ì„œ (ì‹¤ì œ content í¬í•¨)")
        
        return {
            "count": len(docs),
            "documents": docs
        }
    except Exception as e:
        print(f"âŒ Documents list error: {e}")
        traceback.print_exc()
        return {
            "count": 0,
            "documents": []
        }

@router.get("/indexes")
async def list_indexes(request: Request, _user: dict = Depends(get_current_user)):
    """ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë“  RAG ì¸ë±ìŠ¤ ëª©ë¡ ì¡°íšŒ"""
    try:
        enforce_api_rate_limit(
            request,
            bucket="upload-indexes",
            limit=120,
            window_seconds=60,
            user_email=_user.get("email"),
        )
        if is_demo_mode():
            from app.services import demo_store

            indexes = demo_store.list_indexes()
            return {"count": len(indexes), "indexes": indexes}

        from app.services.search_service import get_search_index_client
        
        index_client = get_search_index_client()
        indexes = index_client.list_indexes()
        
        index_list = []
        for index in indexes:
            index_list.append({
                "name": index.name,
                "fields_count": len(index.fields) if index.fields else 0
            })
        
        print(f"ğŸ“‹ ì‚¬ìš© ê°€ëŠ¥í•œ ì¸ë±ìŠ¤: {len(index_list)}ê°œ")
        for idx in index_list:
            print(f"   - {idx['name']}")
        
        return {
            "count": len(index_list),
            "indexes": index_list
        }
    except Exception as e:
        print(f"âŒ Index list error: {e}")
        traceback.print_exc()
        return {
            "count": 0,
            "indexes": []
        }
